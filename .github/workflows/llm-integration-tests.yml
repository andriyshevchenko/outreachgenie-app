name: LLM Integration Tests

on:
  # Only run manually or on schedule to avoid API costs
  workflow_dispatch:
    inputs:
      llm_provider:
        description: 'LLM Provider to test'
        required: false
        default: 'OpenAI'
        type: choice
        options:
          - OpenAI
          - All
  schedule:
    - cron: '0 3 * * 0'  # Run weekly on Sundays at 3 AM UTC

jobs:
  llm-integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    defaults:
      run:
        working-directory: ./server

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup .NET 10
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: 10.0.x

      - name: Verify Docker
        run: |
          docker --version
          docker info

      - name: Restore dependencies
        run: dotnet restore OutreachGenie.slnx

      - name: Build
        run: dotnet build OutreachGenie.slnx --no-restore --configuration Release

      - name: Run LLM Integration Tests
        run: dotnet test OutreachGenie.slnx --no-build --configuration Release --filter "FullyQualifiedName~LlmProviderIntegrationTests" --verbosity normal --logger "trx;LogFileName=llm-integration-tests.trx" --results-directory ./TestResults
        env:
          CI: true
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          # Add other LLM provider keys as needed:
          # ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: llm-integration-test-results
          path: server/TestResults/**/*

      - name: Comment on PR with results
        if: github.event_name == 'workflow_dispatch' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const testResultsPath = 'server/TestResults/llm-integration-tests.trx';
            
            // This is a placeholder - actual implementation would parse TRX and comment
            console.log('LLM integration tests completed');
